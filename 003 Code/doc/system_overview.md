## 시스템 구성 보충 (2~6)

2. 음성 화자 분리는 한 회의 안에서 여러 사람이 동시에 혹은 번갈아 말하는 상황에서, “누가 언제 이야기했는지”를 시간 축 기준으로 정확히 나누어 주는 기능이다. 시스템은 먼저 마이크(또는 시스템 오디오)에서 들어오는 연속적인 음성을 고정 길이의 윈도우로 잘라 처리하고, 각 윈도우에 대해 Silero 기반 음성 활동 검출(VAD)을 적용해 실제로 사람이 말하고 있는 구간만 골라낸다. 이렇게 추려진 구간에 대해서는 SpeechBrain ECAPA‑VoxCeleb 임베딩 모델로 화자의 음성을 고차원 벡터로 변환한 뒤, `SpeakerHandler`를 통해 실시간 클러스터링·분류를 수행한다. 이 과정에서 새 화자는 자동으로 새로운 클러스터(예: “Speaker 3”)로 할당되고, 기존 화자는 과거 임베딩과의 유사도에 따라 재인식된다. 최종적으로는 각 화자별로 “시작 시각–종료 시각–화자 라벨–유사도”로 이루어진 diarization 세그먼트가 생성되며, 이 세그먼트의 시간 정보는 Whisper STT 결과의 timestamp와 매칭되어 “어떤 문장을 어느 화자가 언제 말했는지”까지 연결된다. 이렇게 결합된 전사+화자 정보는 실시간 타임라인 시각화와 화자별 발언 시간·발언 수 집계, 회의 후 보고서의 화자별 분석 등에 공통으로 활용된다.

3. 발언 내용 실시간 평가는 회의 중에 들어오는 각 발언을 AI가 즉시 분석해 “주제에 얼마나 맞는지”와 “얼마나 새로운 아이디어인지”를 수치로 보여주는 기능이다. 이를 위해 시스템은 Gemma 3 계열 모델(로컬 환경에서는 Ollama로 호스팅된 경량 Gemma 모델)을 활용하며, 하나의 프롬프트 안에 회의 주제, 화자 ID, 해당 화자의 직전 발언 컨텍스트, 현재 발언 문장을 함께 넣어 모델이 맥락을 고려한 평가를 수행하도록 설계되어 있다. 모델 출력은 사람이 읽을 수 있는 코멘트와 함께 `(주제일치성, 신규성)` 형태의 점수를 포함하도록 맞추어져 있고, 런타임에서는 이를 파싱해 0~10점 범위의 `topic_relevance`와 `novelty`로 정규화한다. 학습 단계에서는 회의 데이터에서 추출한 텍스트와 내부 평가 결과를 기반으로 각 발언에 대한 정답 레이블(주제 일치성·신규성 점수와 이유 문장)을 준비하고, `prepare_finetune_dataset.py`로 Gemma 전용 학습 데이터셋을 만든 뒤 `train_gemma_lora.py`에서 LoRA 기법을 사용해 Gemma‑3 모델을 회의 평가 태스크에 특화되도록 파인튜닝한다. 서비스 단계에서는 이 평가기가 전사 결과가 들어올 때마다 비동기로 호출되어 실시간 대시보드의 발언 카드와 화자별 통계, 회의 종료 후 자동 보고서에 동일한 점수와 코멘트가 반영되며, 사용자는 회의가 진행되는 동안에도 발언의 질과 주제 적합도를 직관적으로 확인할 수 있다.

4. 백엔드 API 서버는 FastAPI를 기반으로 동작하며, REST API와 WebSocket을 동시에 제공한다. REST API는 회의 세션을 시작하고(`POST /api/start`), 종료하며(`POST /api/stop/{session_id}`), 현재 상태와 통계, 타임라인, 보고서를 조회하는 역할을 담당한다(`GET /api/status`, `/api/speakers`, `/api/timeline`, `/api/report`). 세션이 시작되면 전사 서비스와 화자 분리 서비스가 초기화되고, 회의 주제를 기준으로 AI 평가기(Ollama/Gemma)가 준비된다. WebSocket(`/ws`)은 실시간 전사 결과, 평가 점수, 통계, 타임라인, 보고서 준비 여부를 이벤트 형태로 브라우저에 푸시한다. 서버 내부에서는 `meeting_state`, `meeting_stats`, `meeting_history`, `RollingWaveform` 등 모든 회의 상태를 메모리에서 관리하고, `asyncio.Queue`와 `run_in_executor`를 이용해 음성 처리·평가·브로드캐스트를 비동기적으로 분리함으로써 디스크 I/O와 병목을 최소화한다. 디스크 쓰기는 회의 종료 후 보고서를 PDF로 생성해 저장할 때(`output/meeting_report_<session>.pdf`)에만 발생한다.

5. 실시간 현황판(대시보드)은 Vite + React + TypeScript로 구현된 웹 프런트엔드로, 사용자가 회의의 전체 흐름과 참여자별 기여도를 한눈에 볼 수 있도록 설계되어 있다. 최상위 컴포넌트(App)는 `WebSocketClient`를 통해 백엔드 `/ws`에 연결하고, 동시에 `/api/start`, `/api/stop`, `/api/timeline`, `/api/speakers`, `/api/report`를 호출해 필요한 데이터를 가져온다. 화면 상단 헤더에서는 회의 주제와 인원을 입력하고 회의를 시작·종료하며, 보고서 버튼을 통해 생성된 PDF를 바로 열 수 있다. 중앙의 실시간 발언 영역은 `transcription`·`evaluation` 이벤트를 활용해 화자별 색상과 함께 최근 발언과 AI 평가 점수를 표시하고, 통계 영역은 `stats` 이벤트와 `/api/speakers` 응답을 사용해 전체 평균 점수와 화자별 발언 비율을 시각적으로 보여준다. 타임라인 영역은 `timeline_segment` 이벤트와 `/api/timeline` 데이터로 각 화자의 발언 구간을 시간 축 위에 그려주고, 하단의 분석 테이블은 세션 종료 후 최종 통계를 바탕으로 화자별 발언 수·시간·점수를 정리한다. 레이아웃은 Tailwind 그리드를 사용해 PC·태블릿에서 모두 보기 좋게 구성되어 있으며, 버튼 상태와 색상, 툴팁 등은 WebSocket에서 들어오는 실시간 이벤트에 따라 즉시 갱신된다.

6. 자동 보고서 생성 시스템은 회의 종료 시점의 데이터를 기반으로 AI 요약과 PDF 보고서를 자동으로 만들어 주는 기능이다. 사용자가 회의를 중지하면 서버는 전사·화자 분리 서비스를 정리하고, 그 시점까지의 전체 통계와 화자별 통계, 발언 히스토리를 스냅샷으로 저장한 뒤 `finalize_meeting_report`를 호출한다. 이 함수는 Gemma/Ollama를 이용해 회의 주제와 통계, 최근 발언 로그를 입력으로 한국어 분석 보고서(JSON)를 생성하고, ReportLab 기반 PDF 생성 모듈에 전달하여 표지·개요·통계·발언 로그가 포함된 문서를 `output/meeting_report_<session>.pdf`로 렌더링한다. 생성 결과는 메모리에 캐시되어 `GET /api/report`로 PDF를 바로 내려받거나, `GET /api/report?format=json`으로 요약 데이터만 활용할 수 있다. 이를 통해 사용자는 회의가 끝난 직후 각 화자의 발언 통계와 AI 평가 결과, 회의 개요와 개선 제안이 포함된 보고서를 별도의 수작업 없이 확보하고, 이후 복기나 공유 용도로 쉽게 활용할 수 있다.
